{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "%load_ext tensorboard\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "plt.style.use('./article.mplstyle')\n",
    "\n",
    "# tf.keras.utils.set_random_seed(1)\n",
    "# tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow version and CPU & GPU details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Tensorflow version: {tf.__version__}')\n",
    "# Check GPU availability.\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print('Devices:')\n",
    "for dev in device_lib.list_local_devices():\n",
    "    print(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = '../data'\n",
    "filename = 'synthetic_ppg.npy'\n",
    "synts, ppgs_raw, ppgs, labels, labels_fixed, noises, model_params = np.load(f'{output_folder}/{filename}', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_loss(y_true, y_pred):\n",
    "    y_true /= tf.reduce_sum(y_true)\n",
    "    y_pred /= tf.reduce_sum(y_pred)\n",
    "    loss = tf.reduce_sum(\n",
    "        tf.abs(tf.subtract(tf.cumsum(y_true), tf.cumsum(y_pred))))\n",
    "    return loss\n",
    "\n",
    "# Kazemi et al. article (Robust PPG Peak Detection Using Dilated Convolutional Neural Networks).\n",
    "model_kazemi = tf.keras.Sequential()\n",
    "model_kazemi.add(keras.layers.Input(shape=(model_params.s_len, 1), name='input'))\n",
    "model_kazemi.add(keras.layers.Conv1D(kernel_size=3, filters=4, activation='elu', dilation_rate=1, padding='same'))\n",
    "model_kazemi.add(keras.layers.Conv1D(kernel_size=3, filters=8, activation='elu', dilation_rate=2, padding='same'))\n",
    "model_kazemi.add(keras.layers.Conv1D(kernel_size=3, filters=8, activation='elu', dilation_rate=4, padding='same'))\n",
    "model_kazemi.add(keras.layers.Conv1D(kernel_size=3, filters=16, activation='elu', dilation_rate=8, padding='same'))\n",
    "model_kazemi.add(keras.layers.Conv1D(kernel_size=3, filters=16, activation='elu', dilation_rate=16, padding='same'))\n",
    "model_kazemi.add(keras.layers.Conv1D(kernel_size=3, filters=32, activation='elu', dilation_rate=32, padding='same',))\n",
    "model_kazemi.add(keras.layers.Conv1D(kernel_size=3, filters=1, activation='sigmoid', dilation_rate=64, padding='same'))\n",
    "model_kazemi.compile(optimizer=tf.optimizers.Adam(), loss=nn_loss, metrics=['binary_accuracy'])\n",
    "\n",
    "# Small model.\n",
    "model_small = tf.keras.Sequential()\n",
    "model_small.add(keras.layers.Conv1D(input_shape=(model_params.s_len, 1), kernel_size=5, filters=2, activation='swish', dilation_rate=1, padding='same'))\n",
    "model_small.add(keras.layers.Normalization())\n",
    "model_small.add(keras.layers.Conv1D(kernel_size=5, filters=4, activation='swish', dilation_rate=2, padding='same'))\n",
    "model_small.add(keras.layers.Normalization())\n",
    "model_small.add(keras.layers.Conv1D(kernel_size=5, filters=8, activation='swish', dilation_rate=4, padding='same'))\n",
    "model_small.add(keras.layers.Normalization())\n",
    "model_small.add(keras.layers.Conv1D(kernel_size=5, filters=1, activation='sigmoid', dilation_rate=8, padding='same'))\n",
    "model_small.compile(optimizer=tf.optimizers.Adam(), loss=nn_loss, metrics=['binary_accuracy'])\n",
    "\n",
    "# Tiny model.\n",
    "model_tiny = tf.keras.Sequential()\n",
    "model_tiny.add(keras.layers.Conv1D(input_shape=(model_params.s_len, 1), kernel_size=5, filters=2, activation='swish', dilation_rate=2, padding='same'))\n",
    "model_tiny.add(keras.layers.Normalization())\n",
    "model_tiny.add(keras.layers.Conv1D(kernel_size=5, filters=1, activation='sigmoid', dilation_rate=4, padding='same'))\n",
    "model_tiny.compile(optimizer=tf.optimizers.Adam(), loss=nn_loss, metrics=['binary_accuracy'])\n",
    "\n",
    "model_output_dir = 'models'\n",
    "\n",
    "model_names = ['tiny', 'small', 'kazemi']\n",
    "models = [model_tiny, model_small, model_kazemi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "steps_per_epoch = None\n",
    "epochs = 20\n",
    "nn_labels = labels_fixed\n",
    "\n",
    "# Split the dataset into training (80%), validation (10%) and testing (10%).\n",
    "ds_len = len(ppgs)\n",
    "x_train = ppgs[:int(0.8 * ds_len)]\n",
    "y_train = nn_labels[:int(0.8 * ds_len)]\n",
    "x_val = ppgs[int(0.8 * ds_len):int(0.9 * ds_len)]\n",
    "y_val = nn_labels[int(0.8 * ds_len):int(0.9 * ds_len)]\n",
    "x_test = ppgs[int(0.9 * ds_len):]\n",
    "y_test = nn_labels[int(0.9 * ds_len):]\n",
    "assert len(x_train) + len(x_val) + len (x_test) == ds_len, 'Training, validation and test dataset lengths do not match that of the whole dataset.'\n",
    "\n",
    "for model_name, model in zip(model_names, models):\n",
    "    # Train the model.\n",
    "    csv_logger = keras.callbacks.CSVLogger(f'../{model_output_dir}/{model_name}.log')\n",
    "    log_dir = \"../tensorboard_logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    history = model.fit(x_train, y_train, batch_size=batch_size, \n",
    "        steps_per_epoch=steps_per_epoch, epochs=epochs, validation_data=(x_val, y_val), \n",
    "        callbacks=[csv_logger, tensorboard_callback])\n",
    "\n",
    "    # Print model details.\n",
    "    model.summary()\n",
    "\n",
    "    # Run the test set.\n",
    "    test_res = model.evaluate(x_test, y_test)\n",
    "    print('Test loss, test accuracy:', test_res)\n",
    "\n",
    "    # Plot history.\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(8, 5))\n",
    "    axes[0].set_title('Loss')\n",
    "    axes[0].plot(history.epoch, history.history['loss'], label='Training loss')\n",
    "    axes[0].plot(history.epoch, history.history['val_loss'],\n",
    "                    label='Validation loss')\n",
    "    axes[1].set_title('Accuracy')\n",
    "    axes[1].plot(history.epoch, history.history['binary_accuracy'],\n",
    "                    label='Training accuracy')\n",
    "    axes[1].plot(history.epoch, history.history['val_binary_accuracy'],\n",
    "                    label='Validation accuracy')\n",
    "    axes[0].legend()\n",
    "    axes[1].legend()\n",
    "\n",
    "    # Save the model and convert it into a TF Lite model.\n",
    "    run_model = tf.function(lambda x: model(x))\n",
    "    # This is important, let's fix the input size.\n",
    "    concrete_func = run_model.get_concrete_function(\n",
    "        tf.TensorSpec([1, model_params.s_len, 1], model.inputs[0].dtype))\n",
    "\n",
    "    # Save the model.\n",
    "    model.save(f'../{model_output_dir}/{model_name}', save_format=\"tf\", signatures=concrete_func)\n",
    "\n",
    "    # Convert to TF Lite model.\n",
    "    converter = tf.lite.TFLiteConverter.from_saved_model(f'../{model_output_dir}/{model_name}')\n",
    "    # converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    tflite_model = converter.convert()\n",
    "\n",
    "    # Analyze the model.\n",
    "    tf.lite.experimental.Analyzer.analyze(model_content=tflite_model)\n",
    "    # Save the model.\n",
    "    open(f'../{model_output_dir}/{model_name}.tflite', 'wb').write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model = tf.function(lambda x: model(x))\n",
    "# This is important, let's fix the input size.\n",
    "concrete_func = run_model.get_concrete_function(\n",
    "    tf.TensorSpec([1, model_params.s_len, 1], model.inputs[0].dtype))\n",
    "\n",
    "# Save the model.\n",
    "model.save(f'../{model_output_dir}/{model_name}', save_format=\"tf\", signatures=concrete_func)\n",
    "\n",
    "# Convert to TF Lite model.\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(f'../{model_output_dir}/{model_name}')\n",
    "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Analyze the model.\n",
    "tf.lite.experimental.Analyzer.analyze(model_content=tflite_model)\n",
    "# Save the model.\n",
    "open(f'../{model_output_dir}/{model_name}.tflite', 'wb').write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigate to the models dir and run the following command in e.g. Git Bash to convert the model to C array:\n",
    "```console\n",
    "xxd -i model.tflite > model.cc\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ecef9f3479ecf9ceb2ca5791c50200f285db557ac3e75ea18b1853dc6e2df942"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
